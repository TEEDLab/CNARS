{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9SoFLIx-rF-N"
   },
   "source": [
    "# Text Classification using Classical ML algorithms\n",
    "\n",
    "Run traditional machine learning models on text data for prediction  \n",
    "\n",
    "Author: Jenna Kim & Jinseok Kim \n",
    "Created: 2022/1/12  \n",
    "Last Modified: 2023/10/08 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KjDQGl91wA78"
   },
   "source": [
    "# 1. Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iCQmTfQRrF-S"
   },
   "outputs": [],
   "source": [
    "import timeit\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7OIb10PFrF-U",
    "outputId": "ea872d94-e673-4156-b9e0-9b5a45f5bfc9"
   },
   "outputs": [],
   "source": [
    "# Install nltk\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Install Imbalanced-Learn library for sampling if not already installed\n",
    "!pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "acU18CBUv4xQ"
   },
   "outputs": [],
   "source": [
    "# Hide warning messages from display\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EmfKPvo9wEne"
   },
   "source": [
    "# 2. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pWnA_g5SrF-V"
   },
   "outputs": [],
   "source": [
    "def load_data(filename, colname, record):\n",
    "    \"\"\"\n",
    "    Read in input file and load data\n",
    "    \n",
    "    filename: csv file\n",
    "    colname: column name for texts\n",
    "    record: text file to save summary\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_csv(filename, encoding='utf-8')\n",
    "    \n",
    "    # No of rows and columns\n",
    "    print(\"No of Rows: {}\".format(df.shape[0]), file=record)\n",
    "    print(\"No of Columns: {}\".format(df.shape[1]), file=record)\n",
    "    \n",
    "    print(\"No of Rows: {}\".format(df.shape[0]))\n",
    "    print(\"No of Columns: {}\".format(df.shape[1]))\n",
    "    \n",
    "    # Select data needed for processing & convert labels\n",
    "    df = df[['pmid', 'title', 'abstract', 'pubtype']]\n",
    "    df.iloc[:, -1] = df.iloc[:, -1].map({'RCT':1, 'Other':0})\n",
    "\n",
    "    # Remove null values \n",
    "    df=df.dropna()\n",
    "\n",
    "    print(\"No of rows (After removing null): {}\".format(df.shape[0]), file=record)\n",
    "    print(\"No of columns: {}\".format(df.shape[1]), file=record)\n",
    "    \n",
    "    print(\"No of rows (After removing null): {}\".format(df.shape[0]))\n",
    "    print(\"No of columns: {}\".format(df.shape[1]))\n",
    "\n",
    "    # Select text columns\n",
    "    if colname == \"title\":\n",
    "        df = df[['pmid', 'title', 'pubtype']]\n",
    "        df.rename({\"title\": \"sentence\", \"pubtype\": \"label\"}, axis=1, inplace=True)\n",
    "    elif colname == \"abs\":\n",
    "        df = df[['pmid', 'abstract', 'pubtype']]\n",
    "        df.rename({\"abstract\": \"sentence\", \"pubtype\": \"label\"}, axis=1, inplace=True)\n",
    "    elif colname == \"mix\":\n",
    "        df['mix'] = df[['title','abstract']].apply(lambda x : '{} {}'.format(x[0],x[1]), axis=1)\n",
    "        df = df[['pmid', 'mix', 'pubtype']]\n",
    "        df.rename({\"mix\": \"sentence\", \"pubtype\": \"label\"}, axis=1, inplace=True)\n",
    "\n",
    "    # Check the first few instances\n",
    "    print(\"\\n<Data View: First Few Instances>\", file=record)\n",
    "    print(\"\\n\", df.head(), file=record)\n",
    "    print(\"\\n<Data View: First Few Instances>\")\n",
    "    print(\"\\n\", df.head()) \n",
    "    \n",
    "    # No of lables and rows \n",
    "    print('\\nClass Counts(label, row): Total', file=record)\n",
    "    print(df.label.value_counts(), file=record)\n",
    "    \n",
    "    print('\\nClass Counts(label, row): Total')\n",
    "    print(df.label.value_counts())\n",
    "\n",
    "    # Split into X and y\n",
    "    X, y = df.iloc[:, :-1], df.iloc[:, -1]\n",
    "     \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i6UoMg2jrF-W"
   },
   "outputs": [],
   "source": [
    "def sample_data(X_train, y_train, record, sampling=0, sample_method='over'):\n",
    "    \"\"\"\n",
    "       Sampling input train data\n",
    "       \n",
    "       X_train: dataframe of X train data\n",
    "       y_train: datafram of y train data\n",
    "       sampling: indicator of sampling funtion is on or off\n",
    "       sample_method: method of sampling (oversampling or undersampling)\n",
    "       record: text file to save summary\n",
    "       \n",
    "    \"\"\"\n",
    "    \n",
    "    from imblearn.over_sampling import RandomOverSampler\n",
    "    from imblearn.under_sampling import RandomUnderSampler\n",
    "    \n",
    "    if sampling:\n",
    "        # select a sampling method\n",
    "        if sample_method == 'over':\n",
    "            oversample = RandomOverSampler(random_state=42)\n",
    "            X_over, y_over = oversample.fit_resample(X_train, y_train)\n",
    "            print('\\n****** Data Sampling ******', file=record)\n",
    "            print('\\nOversampled Data (class, Rows):\\n{}'.format(y_over.value_counts()), file=record)\n",
    "            print('\\nOversampled Data (class, Rows):\\n{}'.format(y_over.value_counts()))\n",
    "            X_train_sam, y_train_sam = X_over, y_over\n",
    "            \n",
    "        elif sample_method == 'under':\n",
    "            undersample = RandomUnderSampler(random_state=42)\n",
    "            X_under, y_under = undersample.fit_resample(X_train, y_train)\n",
    "            print('\\n****** Data Sampling ******', file=record)\n",
    "            print('\\nUndersampled Data (class,Rows):\\n{}'.format(y_under.value_counts()), file=record)\n",
    "            print('\\nUndersampled Data (class,Rows):\\n{}'.format(y_under.value_counts()))\n",
    "            X_train_sam, y_train_sam = X_under, y_under\n",
    "    else:\n",
    "        X_train_sam, y_train_sam = X_train, y_train \n",
    "        print('\\n****** Data Sampling ******', file=record)\n",
    "        print('\\nNo Sampling Performed\\n', file=record)\n",
    "    \n",
    "    return X_train_sam, y_train_sam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kx6Q8w3lrF-X"
   },
   "outputs": [],
   "source": [
    "def preprocess_data(X_data_raw):\n",
    "    \"\"\"\n",
    "       Preprocess data with lowercase conversion, punctuation removal, tokenization, stemming\n",
    "       \n",
    "       X_data_raw: X data in dataframe\n",
    "       \n",
    "    \"\"\"\n",
    "    \n",
    "    X_data=X_data_raw.iloc[:, -1].astype(str)\n",
    "   \n",
    "    # .1 convert all characters to lowercase\n",
    "    X_data = X_data.map(lambda x: x.lower())\n",
    "    \n",
    "    # 2. remove punctuation\n",
    "    X_data = X_data.str.replace('[^\\w\\s]', '')\n",
    "    \n",
    "    # 3. word tokenize\n",
    "    X_data = X_data.apply(nltk.word_tokenize)\n",
    "    \n",
    "    # 4. stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    X_data = X_data.apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "    \n",
    "    # 5. removing unnecessary space\n",
    "    X_data = X_data.apply(lambda x: ' '.join(x)) \n",
    "    \n",
    "    return X_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Nnyw5kYrF-X"
   },
   "outputs": [],
   "source": [
    "def fit_model(X_train, y_train, model='DT'):\n",
    "    \n",
    "    \"\"\"\n",
    "      Model fitting with options of classifiers:\n",
    "      decision tree, svm, knn, naive bayes, random forest, and gradient boosting\n",
    "      \n",
    "      X_train: X train data\n",
    "      y_train: y train data\n",
    "      model: name of classifier\n",
    "      \n",
    "    \"\"\"\n",
    "    \n",
    "    if model=='DT':\n",
    "        DT = DecisionTreeClassifier(max_depth=2)\n",
    "        model = DT.fit(X_train, y_train)\n",
    "    elif model=='SVM':\n",
    "        SVM = SVC(kernel='linear', probability=True)  \n",
    "        model = SVM.fit(X_train, y_train)\n",
    "    elif model=='NB':\n",
    "        NB = MultinomialNB()\n",
    "        model = NB.fit(X_train, y_train)\n",
    "    elif model=='LR':\n",
    "        LR = LogisticRegression()\n",
    "        model = LR.fit(X_train, y_train)   \n",
    "    elif model=='RF':\n",
    "        RF = RandomForestClassifier(max_depth=2, random_state=0)\n",
    "        model = RF.fit(X_train, y_train)\n",
    "    elif model=='GB':\n",
    "        GB = GradientBoostingClassifier()\n",
    "        model = GB.fit(X_train, y_train)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ctZ80-2prF-Y"
   },
   "outputs": [],
   "source": [
    "def evaluate_model(y_test, y_pred, record, eval_model=0):\n",
    "    \"\"\"\n",
    "      evaluate model performance\n",
    "      \n",
    "      y_test: y test data\n",
    "      y_pred: t prediction score\n",
    "      eval_model: indicator if this funtion is on or off\n",
    "      \n",
    "    \"\"\"\n",
    "    \n",
    "    if eval_model:\n",
    "        print('\\n************** Model Evaluation **************', file=record)\n",
    "        print('\\n************** Model Evaluation **************')\n",
    "        \n",
    "        print('\\nConfusion Matrix:\\n', file=record)\n",
    "        print(confusion_matrix(y_test, y_pred), file=record)\n",
    "        print('\\nConfusion Matrix:\\n')\n",
    "        print(confusion_matrix(y_test, y_pred))\n",
    "    \n",
    "        print('\\nClassification Report:\\n', file=record)\n",
    "        print(classification_report(y_test, y_pred, digits=4), file=record)\n",
    "        print('\\nClassification Report:\\n')\n",
    "        print(classification_report(y_test, y_pred, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H8ikM_EPrF-Z"
   },
   "outputs": [],
   "source": [
    "def predict_proba(model, X_test_trans, X_test, y_test, y_pred, proba_file, proba_out=0):\n",
    "    \"\"\"\n",
    "       Predict probability of each class\n",
    "       \n",
    "       model: trained model with a selected classifier\n",
    "       X_test_trans: X test data preprocessed\n",
    "       X_test: original X test data\n",
    "       y_test: original y test data\n",
    "       y_pred: predicted y values\n",
    "       proba_file: output file of probability scores\n",
    "       proba_out: decide if the probability output is expected\n",
    "       \n",
    "    \"\"\"\n",
    "    if proba_out:\n",
    "      \n",
    "        ## Compute probability\n",
    "        y_prob = model.predict_proba(X_test_trans)\n",
    "        df_prob = pd.DataFrame(data=y_prob, columns=model.classes_)\n",
    "        result = pd.concat([X_test.reset_index(drop=True), df_prob], axis=1, ignore_index=False)\n",
    "    \n",
    "        ## Add predicted class to output\n",
    "        result['pred'] = pd.Series(y_pred)\n",
    "\n",
    "        ## Add actual class to output \n",
    "        y_test = y_test.reset_index(drop=True)\n",
    "        result['act'] = y_test\n",
    "\n",
    "        ## Save output\n",
    "        result.to_csv(proba_file, encoding='utf-8', index=False, header=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jtuA57AQwTjA"
   },
   "source": [
    "# 3. Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5YROMNEZrF-Z"
   },
   "outputs": [],
   "source": [
    "def main(input_file, \n",
    "         colname,   \n",
    "         sample_on, \n",
    "         sample_type, \n",
    "         model_method, \n",
    "         eval_on, \n",
    "         proba_file,\n",
    "         proba_on,\n",
    "         result_file,\n",
    "         datasize_change,\n",
    "         ratio):\n",
    "    \n",
    "    \"\"\"\n",
    "       Main function for processing data, model fitting, and prediction\n",
    "       \n",
    "       input_file: input file\n",
    "       colname: colume name for selection between title and abstract\n",
    "       sample_on: indicator of sampling on or off\n",
    "       sample_type: sample type to choose if sample_on is 1\n",
    "       model_method: name of classifier to be applied for model fitting\n",
    "       eval_on: indicator of model evaluation on or off\n",
    "       proba_file: name of output file of probability\n",
    "       proba_on: indicator of getting probability\n",
    "       result_file: name of output file of evaluation\n",
    "       datasize_change: indication of data size change\n",
    "       ratio: proportion of data size\n",
    "       \n",
    "    \"\"\"\n",
    "    ## 0. open result file for records\n",
    "    f=open(result_file, \"a\")\n",
    "    \n",
    "    ## 1. Load data\n",
    "    \n",
    "    print(\"\\n************** Loading Data ************\\n\", file=f)\n",
    "    print(\"\\n************** Loading Data ************\\n\")\n",
    "    X, y = load_data(input_file, colname, record=f)\n",
    "    \n",
    "    # testing\n",
    "    print(\"\\n<First Sentence>\\n{}\".format(X.sentence[0]), file=f)\n",
    "    print(\"\\n<First Sentence>\\n{}\".format(X.sentence[0]))\n",
    "\n",
    "    ## 2. Train and test split\n",
    "    \n",
    "    print(\"\\n************** Spliting Data **************\\n\", file=f)\n",
    "    print(\"\\n************** Spliting Data **************\\n\")\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    X_val, X_test, y_val, y_test = train_test_split(X_test,y_test, test_size=0.5, random_state=42, stratify=y_test)\n",
    "    \n",
    "    # For testing only: small size data\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.99, random_state=42, stratify=y)\n",
    "    #X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.99, random_state=42, stratify=y_test)\n",
    "    #X_notuse, X_test, y_notuse, y_test = train_test_split(X_test, y_test, test_size=0.01, random_state=42, stratify=y_test)\n",
    "    \n",
    "    print(\"Train Data: {}\".format(X_train.shape), file=f)\n",
    "    print(\"Val Data: {}\".format(X_val.shape), file=f)\n",
    "    print(\"Test Data: {}\".format(X_test.shape), file=f)\n",
    "    \n",
    "    print(\"Train Data: {}\".format(X_train.shape))\n",
    "    print(\"Val Data: {}\".format(X_val.shape))\n",
    "    print(\"Test Data: {}\".format(X_test.shape))\n",
    "    \n",
    "    print('\\nClass Counts(label, row): Train', file=f)\n",
    "    print(y_train.value_counts(), file=f)\n",
    "    print('\\nClass Counts(label, row): Test', file=f)\n",
    "    print(y_test.value_counts(), file=f)\n",
    "    \n",
    "    print(\"\\n<X_test Data>\", file=f)\n",
    "    print(X_test.head(), file=f)\n",
    "    print(\"\\n<X_test Data>\")\n",
    "    print(X_test.head())\n",
    "    \n",
    "    ## 3. Data size change\n",
    "    \n",
    "    if datasize_change:\n",
    "        \n",
    "        print(\"\\n************** Data Size Change *************\\n\", file=f)\n",
    "        print(\"Data Ratio (size): {} ({})\".format(ratio, int(X_train.shape[0]*ratio)), file=f)\n",
    "        print(\"\\n************** Data Size Change *************\\n\")\n",
    "        print(\"Data Size: {} ({})\".format(ratio, int(X_train.shape[0]*ratio)))\n",
    "        \n",
    "        X_train, _, y_train, _ = train_test_split(X_train, y_train, test_size=(1-ratio), random_state=42, stratify=y_train)  \n",
    "    \n",
    "    # Reset index\n",
    "    X_train=X_train.reset_index(drop=True)\n",
    "    X_test=X_test.reset_index(drop=True)\n",
    "    y_train=y_train.reset_index(drop=True)\n",
    "    y_test=y_test.reset_index(drop=True)\n",
    "    \n",
    "    print(\"\\n************** Processing Data **************\", file=f)\n",
    "    print(\"\\n************** Processing Data **************\")\n",
    "    print(\"\\nTrain Data: {}\".format(X_train.shape), file=f)\n",
    "    print(\"Test Data: {}\".format(X_test.shape), file=f)\n",
    "    print(\"\\nTrain Data: {}\".format(X_train.shape))\n",
    "    print(\"Test Data: {}\".format(X_test.shape))\n",
    "    \n",
    "    print('\\nClass Counts(label, row): Train', file=f)\n",
    "    print(y_train.value_counts(), file=f)\n",
    "    print('\\nClass Counts(label, row): Test', file=f)\n",
    "    print(y_test.value_counts(), file=f)\n",
    "    \n",
    "    print(\"\\n<X_test Data>\", file=f)\n",
    "    print(X_test.head(), file=f)\n",
    "    print(\"\\n<X_test Data>\")\n",
    "    print(X_test.head())\n",
    "    \n",
    "    ## 4. Sampling \n",
    "    X_train_samp, y_train_samp = sample_data(X_train, y_train, record=f, sampling=sampling_on, sample_method=sample_type)\n",
    "    \n",
    "    ## 5. Preprocessing \n",
    "    X_train_pro = preprocess_data(X_train_samp)\n",
    "    \n",
    "    # TFIDF transformation\n",
    "    count_vect = CountVectorizer()\n",
    "    counts = count_vect.fit_transform(X_train_pro)\n",
    "    transformer = TfidfTransformer(smooth_idf=True, use_idf=True).fit(counts)\n",
    "    X_train_transformed = transformer.transform(counts)\n",
    "    \n",
    "    X_train_trans = X_train_transformed\n",
    "    y_train_trans = y_train_samp\n",
    "\n",
    "    ## 6. Model Fitting\n",
    "    print(\"\\n************** Training Model: \" + model_method + \" **************\", file=f)\n",
    "    print(\"\\n************** Training Model: \" + model_method + \" **************\")\n",
    "\n",
    "    # Check training time\n",
    "    start_time = timeit.default_timer()\n",
    "    \n",
    "    # Fit the model\n",
    "    model = fit_model(X_train_trans, y_train_trans, model=model_method)\n",
    "    \n",
    "    elapsed = timeit.default_timer() - start_time\n",
    "    print(\"\\nTraining Time: {}\".format(round(elapsed, 2)), file=f)\n",
    "    print(\"\\nTraining Time: {}\".format(round(elapsed,2)))\n",
    "\n",
    "    ## 7. Prediction\n",
    "    print(\"\\n\\n************** Getting predictions **************\", file=f)\n",
    "    print(\"\\n\\n************** Getting predictions **************\")\n",
    "\n",
    "    # Transform X_test data\n",
    "    X_test_pro = preprocess_data(X_test)\n",
    "    counts_test = count_vect.transform(X_test_pro)\n",
    "    X_test_trans = transformer.transform(counts_test)\n",
    "    \n",
    "    # Predict output\n",
    "    y_pred = model.predict(X_test_trans)\n",
    "    \n",
    "    ## 8. Evaluating model performance\n",
    "    print(\"\\n************** Evaluating performance **************\", file=f)\n",
    "    print(\"\\n************** Evaluating performance **************\")\n",
    "    evaluate_model(y_test, y_pred, record=f, eval_model=eval_on)\n",
    "    \n",
    "    ## 9. Probability prediction    \n",
    "    predict_proba(model, X_test_trans, X_test, y_test, y_pred, proba_file=proba_file, proba_out=proba_on)\n",
    "    \n",
    "    print(\"\\nOutput file:'\" + result_file + \"' Created\", file=f)\n",
    "    print(\"\\nOutput file:'\" + result_file + \"' Created\")\n",
    "    \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gYwmardMwKPF"
   },
   "source": [
    "# 4. Run code for implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pYt_qbIcrF-a",
    "outputId": "3174ef17-7ebc-440c-d280-0d53271924f5"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "if __name__== \"__main__\":\n",
    "    \n",
    "    ###### 1. Set Parameter Values ######\n",
    "    \n",
    "    #### 1-1. Input file name & which column\n",
    "    input_filename=\"rct_sample.csv\"  \n",
    "    column_name = \"abs\"                                        # 'title' for title text; 'abs' for abstract; 'mix' for title + abstract\n",
    "\n",
    "    #### 1-2. Data size change?\n",
    "    datachange_on=1                                            # 0 for no change; 1 for change of data size\n",
    "    ratio_list=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]   # ratio for data size\n",
    "    #ratio_list=[0.1]  # for testing\n",
    "    \n",
    "    #### 1-3. Sampling applied?\n",
    "    sampling_on=0                                              # 0 for no sampling; 1 for sampling\n",
    "    sampling_type='over'                                       # Use when sampling_on=1; 'over'(oversampling), 'under'(undersampling)\n",
    "    \n",
    "    #### 1-4. Which model to use?\n",
    "    model_type='LR'                                            # 'LR'(Logisitic regression);SVM'(SVM);'NB'(Naive Bayes);\n",
    "                                                               # 'RF'(Random Forest);'GB'(Gradient Boosting)\n",
    "    #### 1-5. Evaluation & probability file    \n",
    "    eval_on=1                                                  # 0 for no; 1 for yes (confusion matrix/classification report)\n",
    "    proba_on=0                                                 # 0 for no; 1 for yes (probability output)\n",
    "    \n",
    "    \n",
    "    ###### 2. Run Main Fuction ######\n",
    "\n",
    "    if datachange_on:            \n",
    "        \n",
    "        for ratio in ratio_list:           \n",
    "            if sampling_on:\n",
    "                proba_file = \"result_ml_\" + str(ratio) + \"_\" + model_type + \"_\" + sampling_type + \"_\" + column_name + \".csv\" \n",
    "                eval_file = \"eval_ml_\" + str(ratio) + \"_\" + model_type + \"_\" + sampling_type + \"_\" + column_name + \".txt\" \n",
    "            else:\n",
    "                proba_file = \"result_ml_\" + str(ratio) + \"_\" + model_type + \"_\" + column_name + \".csv\"   \n",
    "                eval_file = \"eval_ml_\" + str(ratio) + \"_\" + model_type + \"_\" + column_name + \".txt\"\n",
    "            main(input_file=input_filename,\n",
    "                 colname=column_name, \n",
    "                 sample_on=sampling_on, \n",
    "                 sample_type=sampling_type,\n",
    "                 model_method=model_type, \n",
    "                 eval_on=eval_on, \n",
    "                 proba_file=proba_file,\n",
    "                 proba_on=proba_on,\n",
    "                 result_file=eval_file,\n",
    "                 datasize_change=datachange_on,\n",
    "                 ratio=ratio)\n",
    "    else:\n",
    "        if sampling_on:\n",
    "            proba_file = \"result_ml_all_\" + model_type + \"_\" + sampling_type + \"_\" + column_name + \".csv\"    \n",
    "            eval_file = \"eval_ml_all_\" + model_type + \"_\" + sampling_type + \"_\" + column_name + \".txt\" \n",
    "        else:\n",
    "            proba_file = \"result_ml_all_\" + model_type + \"_\" + column_name + \".csv\" \n",
    "            eval_file = \"eval_ml_all_\" + model_type + \"_\" + column_name + \".txt\" \n",
    "            \n",
    "        main(input_file=input_filename, \n",
    "             colname=column_name,\n",
    "             sample_on=sampling_on, \n",
    "             sample_type=sampling_type,\n",
    "             model_method=model_type, \n",
    "             eval_on=eval_on, \n",
    "             proba_file=proba_file,\n",
    "             proba_on=proba_on,\n",
    "             result_file=eval_file,\n",
    "             datasize_change=datachange_on,\n",
    "             ratio=1)\n",
    "        \n",
    "    print(\"\\n************** Processing Completed **************\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QftyM5zKrF-b"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "RCT_ML_V1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
